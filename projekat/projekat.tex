\UseRawInputEncoding
% !TEX TS-program = pdflatex
% !TEX encoding = UTF-8 Unicode


\documentclass[11pt]{article} % use larger type; default would be 10pt

%\usepackage[utf8]{inputenc} % set input encoding (not needed with XeLaTeX)


%%% PAGE DIMENSIONS
\usepackage{geometry}
\geometry{a4paper} 

\usepackage{graphicx} 

% \usepackage[parfill]{parskip} % Activate to begin paragraphs with an empty line rather than an indent

%%% PACKAGES

\usepackage{setspace}
\usepackage{listings}
\usepackage{hyperref}
\usepackage{multirow}
\usepackage{changepage} 
\hypersetup{
    colorlinks=true,
    linkcolor=blue,
    filecolor=magenta,      
    urlcolor=cyan,
}

\usepackage{color}
\definecolor{dkgreen}{rgb}{0,0.6,0}
\definecolor{gray}{rgb}{0.5,0.5,0.5}
\definecolor{mauve}{rgb}{0.58,0,0.82}

\lstset{frame=L,
  language=Python,
  aboveskip=3mm,
  belowskip=3mm,
  showstringspaces=false,
  columns=flexible,
  basicstyle={\small\ttfamily},
  numbers=none,
  numberstyle=\tiny\color{gray},
  keywordstyle=\color{blue},
  commentstyle=\color{dkgreen},
  stringstyle=\color{mauve},
  breaklines=true,
  breakatwhitespace=true,
  tabsize=3
}
\usepackage{booktabs} % for much better looking tables
\usepackage{array} % for better arrays (eg matrices) in maths
\usepackage{paralist} % very flexible & customisable lists (eg. enumerate/itemize, etc.)
\usepackage{verbatim} % adds environment for commenting out blocks of text & for better verbatim
\usepackage{subfig} % make it possible to include more than one captioned figure/table in a single float
% These packages are all incorporated in the memoir class to one degree or another...

%%% HEADERS & FOOTERS
\usepackage{fancyhdr} % This should be set AFTER setting up the page geometry
\pagestyle{fancy} % options: empty , plain , fancy
\renewcommand{\headrulewidth}{0pt} % customise the layout...
\lhead{}\chead{}\rhead{}
\lfoot{}\cfoot{\thepage}\rfoot{}

%%% SECTION TITLE APPEARANCE
\usepackage{sectsty}
\allsectionsfont{\sffamily\mdseries\upshape} % (See the fntguide.pdf for font help)
% (This matches ConTeXt defaults)

%%% ToC (table of contents) APPEARANCE
\usepackage[nottoc,notlof,notlot]{tocbibind} % Put the bibliography in the ToC
\usepackage[titles,subfigure]{tocloft} % Alter the style of the Table of Contents
\renewcommand{\cftsecfont}{\rmfamily\mdseries\upshape}
\renewcommand{\cftsecpagefont}{\rmfamily\mdseries\upshape} % No bold!

% wide page for side by side figures, tables, etc
\newlength{\offsetpage}
\setlength{\offsetpage}{1.0cm}
\newenvironment{widepage}{\begin{adjustwidth}{-\offsetpage}{-\offsetpage}%
    \addtolength{\textwidth}{2\offsetpage}}%
{\end{adjustwidth}}

% use serbian latin letters
\usepackage[T1]{fontenc}
\usepackage[utf8]{inputenc}
\usepackage[serbian]{babel}

%%% END Article customizations

%%% The "real" document content comes below...

\title{Rešavanje problema skupa atributa uz pomoć genetskog algoritma}
\author{Anja Miletić}
%\date{} % Activate to display a given date or no date (if empty),
         % otherwise the current date is printed 

\begin{document}
\pagenumbering{gobble}
\maketitle
\newpage

\doublespacing
\tableofcontents
\singlespacing
\newpage

\pagenumbering{arabic}

\section{Uvod}
Problem biranja najmanjeg podskupa atributa (eng. Minimum feature subset selection problem) je bitan u istraživanju podataka i mašinskom učenju. Količina podataka je sve veća, pa su tehnike čišćenja, odnosno smanjivanja obimnosti podataka važnije. U slučaju najmanjeg podskupa atributa, cilj je smanjiti dimenzionalnost podataka bez gubljenja tačnosti klasifikacije, radi bržeg treniranja neuronskih mreža. 

\section{Metode}
Dati problem je NP težak, i teško ga je aproksimirati \cite{VanHorn}. Postoje različiti pristupi rešavanju problema. Nabrojimo neke \cite{Kira}:
\begin{itemize}
\item selekcija pojedinačnih atributa koristeći metrike koje se odnose na važnost tih atributa. Naravno ova metoda je brža i deluje jednostavno, ali ne uzima u obzir odnos između atributa, samim tim ne daje najbolje rezultate.
\item potpuna pretraga prostora rešenja. Od svih valjanih podskupova atributa izabrati onaj koji maksimizuje tačnost. Ova metoda je optimalna - od svih mogućih rešenja ona će dati najmanji podskup.  Zbog velikog broja mogućih rešenja, potpuna pretraga se može koristiti samo ako je ukupan broj atributa, odnosno ukupan broj mogućih podskupova mali
\item heurističke metode pretrage. Sequential Forward Selection (SFS) i Sequential Backward Selection (SBS) algoritmi koriste heuristiku: 'Najbolji atribut za dodavanje u svakom koraku je atribut koji treba izabrati'. Ova heuristika ne uzima obzir interakciju izmedju atributa.
\item Relief algoritam. Baziran na težinama atributa, algoritam prepoznaje one atribute koji su statistički bitni, proučavajuci razlike izmedju vrednosti podataka. Algoritam je polinomijalne složenosti.
\end{itemize} 

\section{Predlog rešenja korišćenjem genetskog algoritma}
Genetski algoritmi pripadaju porodici optimizacionih algoritama koji traže globalni minimum fitnes funkcije. U opstem slučaju, to uključuje četiri koraka: \cite{Cui}
\begin{itemize}
\item evaluacija: bira se pseudo-slučajan set početne populacije. Izračunava se fitnes svake jedinke, zatim se one sortiraju na osnovu fitnesa.
\item reprodukcija: biraju se najbolje jedinke koje se čuvaju u narednoj generaciji. Ove jedinke se zovu elitna deca.
\item rekombinacija: biraju se jedinke koje će se ukrstiti da bi se napravile jedinke za sledeću generaciju. U ovom koraku pokušavamo da sačuvamo najbolje gene i kombinujemo ih da napravimo još bolje jedinke.
\item mutacija: mali procenat populacije prolazi kroz mutaciju (izmenu dela koda). Ovaj korak je bitan da bi se izbeglo zaglavljivanje u lokalnom minimumu.
\end{itemize}

\subsection{Postavka rešenja}
Koristićemo genetski algoritam da nađemo najmanji skup atributa sa najvećom preciznošću. Jedinke će biti kodirane binarnim nizom dužine \textit{m}, gde je \textit{m} ukupan broj atributa našeg seta podataka. Ako je na i-toj poziciji u nizu vrednost \lstinline{True}, onda u podskup atributa ulazi i-ti atribut. Fitnes jedinke predstavlja preciznost klasifikacije modela koji je učen nad podacima koji sadrze podskup atributa predstavljen kodom jedinke. Koristićemo sekvencijalni model \lstinline{keras} biblioteke koji je optimizovan za probleme klasifikacije.
\subsubsection{Podaci}
Koristićemo podatke sa košarkaskih utakmica NBA sezone 2020-21. Atributi se odnose na razne statističke parametre koji se prate tokom utakmice, a zabeleženi su tokom prvog poluvremena. Klasifikikaciju radimo u odnosu na to da li je tim pobedio ili izgubio, pa zbog toga ne gledamo parametre sa kraja utakmice. Nakon čišćenja podataka ostaje nam 21 numerički atribut, kao i kolona \lstinline{W\\L} koju mapiramo kao \lstinline{W->1, L->0}. \newline

\subsubsection{Algoritam}
U nastavku sledi kod za određivanje fitnesa jedinke:

\begin{lstlisting}
def getModelAccuracy(featuresUsed, data):
    # get array of True value indexes, eg [True, False, True] -> [0, 2]
    remainingFeatures = [x for x in range(len(featuresUsed)) if featuresUsed[x]]
    data_final = data.iloc[:, remainingFeatures]
    
    # create model
    X_train, X_test, y_train, y_test = train_test_split(data_final__, results_final, test_size=0.33, random_state=7, stratify=results_final)
    scaler = StandardScaler()
    scaler.fit(X_train)
    X_train = scaler.transform(X_train)
    X_test = scaler.transform(X_test)
    
    model = Sequential()
    model.add(Dense(input_dim=X_train.shape[1], units=500, activation='relu', kernel_constraint=unit_norm()))
    model.add(Dropout(rate=0.2))
    model.add(Dense(units=100, activation='relu', kernel_constraint=unit_norm()))
    model.add(Dense(units=1, activation='sigmoid'))
    model.compile(optimizer='adam', loss='binary_crossentropy', metrics=['accuracy'])
    
    history = model.fit(X_train, y_train, batch_size=64, epochs=20, verbose=1, validation_split=0.3)
    return history.history['val_accuracy'][-1]
\end{lstlisting}

Koristimo metriku \lstinline{val_accuracy}, tačnost klasifikacije validacionog skupa, da bismo izbegli slučajeve gde je \lstinline{accuracy = 100.0} zbog preprilagođavanja.\newline
Klasa \lstinline{Individual} sadrži podatke o jedinki, kao i metode jedinke koje koristimo tokom algoritma:

\begin{lstlisting}
class Individual():
    def __init__(self, numResources):
        # code is a binary array where 0 represents the absence of a feature
        self.code = [random.random() < 0.5 for _ in range(numResources)]
        self.correctNonFeasible()
        
        self.fitness = self.calculateFitness()
        
    def __lt__(self, other):
        if (self.fitness == other.fitness):
            # the smaller the code, the better (less features)
            return len([x for x in self.code if x]) < len([x for x in other.code if x])
        
        # a better individual has a higher accuracy
        return self.fitness > other.fitness
        
    def invert(self):
        i = random.randrange(len(self.code))
        self.code[i] = not self.code[i]
        if self.isFeasible():
            return i
        return -1
    
    def isFeasible(self):
        for c in self.code:
            if c:
                return True
        return False
        
    def correctNonFeasible(self):
        for c in self.code:
            if c:
                return
        # at least one feature must be present
        index = random.randrange(0, len(self.code))
        self.code[index] = True
        
    def calculateFitness(self):
        return getModelAccuracy(self.code)
\end{lstlisting}

Ukoliko je fitnes dve jedinke isti, nama je vrednija ona sa manjim skupom atributa. Skup atributa ne sme da bude prazan, tako da u tom slučaju biramo nasumično jedan atribut koji će pripadati skupu.\newline
Uvodimo pomoćne funkcije selekcije, ukrštanja i mutacije:

\begin{lstlisting}
def selection(population):
    TOURNAMENT_SIZE = 6
    maxAccuracy = float('-inf')
    bestIndex = -1
    
    for i in range(TOURNAMENT_SIZE):
        index = random.randrange(len(population))
        if population[index].fitness > maxAccuracy:
            maxAccuracy = population[index].fitness
            bestIndex = index
    
    return bestIndex
    
def crossover(parent1, parent2, child1, child2):
    breakpoint = random.randrange(0, len(parent1.code))
    
    child1.code[:breakpoint] = parent1.code[:breakpoint]
    child2.code[:breakpoint] = parent2.code[:breakpoint]
    
    child1.code[breakpoint:] = parent2.code[breakpoint:]
    child2.code[breakpoint:] = parent1.code[breakpoint:]
    
    child1.correctNonFeasible()
    child2.correctNonFeasible()
    
def mutation(individual):
    MUTATION_PROB = 0.05
    for i in range(len(individual.code)):
        if random.random() < MUTATION_PROB:
            individual.code[i] = not individual.code[i]
            
    individual.correctNonFeasible()
\end{lstlisting}

Koristićemo turnirsku selekciju i jednopoziciono ukrštanje. Slede definicije samog algortima:

\begin{lstlisting}
def genAlgWithoutSimulatedAnnealing():
    POPULATION_SIZE = 20
    numResources = data_final.shape[1]
    population = [Individual(numResources) for i in range(POPULATION_SIZE)]
    newPopulation = [Individual(numResources) for i in range(POPULATION_SIZE)]

    ELITISM_SIZE = int(0.3 * POPULATION_SIZE)
    MAX_ITER = 30
    for i in range(MAX_ITER):
        population.sort()
        newPopulation[:ELITISM_SIZE] = population[:ELITISM_SIZE]
        for j in range(ELITISM_SIZE, POPULATION_SIZE-1, 2):
            parent1Index = selection(population)
            parent2Index = selection(population)
            
            crossover(population[parent1Index], population[parent2Index], newPopulation[j], newPopulation[j+1])

            mutation(newPopulation[j])
            mutation(newPopulation[j+1])

            newPopulation[j].fitness = newPopulation[j].calculateFitness()
            newPopulation[j + 1].fitness = newPopulation[j + 1].calculateFitness()

        population = newPopulation

    bestIndividual = max(population, key=lambda x: x.fitness)
    print('Solution: {}, fitness: {}'.format(bestIndividual.code, bestIndividual.fitness))
    
\end{lstlisting}

\subsection{Varijacije}
\subsubsection{Simulirano kaljenje}

Pored običnog algoritma implementiraćemo i algoritam sa simuliranim kaljenjem. Simulirano kaljenje je još jedna tehnika koja omogućuje izlazak iz lokalnog minimuma, tako što se prihvata gore rešenje, sa verovatnoćom obrnuto-proporcionalnom broju iteracija.

\begin{lstlisting}
    
def simulatedAnnealing(individual, iters):
    for i in range(iters):
        j = individual.invert()
        if j < 0:
            continue
        newFitness = individual.calculateFitness()
        
        if newFitness > individual.fitness:
            individual.fitness = newFitness
        else:
            p = 1.0 / (i + 1) ** 0.5
            q = random.uniform(0, 1)
            if p < q:
                individual.fitness = newFitness
            else:
                individual.code[j] = not individual.code[j]
                
def genAlgSimulatedAnnealing():
    POPULATION_SIZE = 20
    numResources = data_final.shape[1]
    population = [Individual(numResources) for i in range(POPULATION_SIZE)]
    newPopulation = [Individual(numResources) for i in range(POPULATION_SIZE)]

    ELITISM_SIZE = int(0.3 * POPULATION_SIZE)
    MAX_ITER = 30
    for i in range(MAX_ITER):
        population.sort()
        newPopulation[:ELITISM_SIZE] = population[:ELITISM_SIZE]
        for j in range(ELITISM_SIZE, POPULATION_SIZE, 2):
            parent1Index = selection(population)
            parent2Index = selection(population)

            crossover(population[parent1Index], population[parent2Index], newPopulation[j], newPopulation[j+1])

            mutation(newPopulation[j])
            mutation(newPopulation[j+1])

            newPopulation[j].fitness = newPopulation[j].calculateFitness()
            newPopulation[j + 1].fitness = newPopulation[j + 1].calculateFitness()


        simulatedAnnealing(newPopulation[0], 10)
        population = newPopulation

    bestIndividual = max(population, key=lambda x: x.fitness)
    print('Solution: {}, fitness: {}'.format(bestIndividual.code, bestIndividual.fitness))
\end{lstlisting}

\subsubsection{Fitnes funkcija}
Ono što se javlja kao problem u oba navedena algoritma je što fitnes funkcija isključivo zavisi od preciznosti modela. Može da se desi da model koji ima malo bolju preciznost sadrži mnogo više atributa, ali je klasifikovan kao bolji. Zbog toga uvodimo i broj atributa u računanje fitnes funkcije. Koristeći \lstinline{StandardScaler} možemo da kvantifikujemo ove vrednosti - jako male i jako velike vrednosti će dovoljno uticati na fitnes, tako da dajemo veću prednost jedinkama sa manjim brojem atributa. Sledi nova funkcija za izračunavanje fitnesa:
\begin{lstlisting}
def calculateFitness(code, accuracy):
    scaler = StandardScaler()
    max_attributes = len(code)
    num_attributes = len([x for x in code if x])
    arr = np.array([1.0, num_attributes, max_attributes]).reshape(-1, 1)
    scaler.fit(arr)
    arr = scaler.transform(arr)
    [_, penalty, _] = scaler.fit_transform(X=arr)
    return accuracy - penalty
\end{lstlisting}

\section{Rezultati}
Uporedićemo rešenja za malo \lstinline{n} koja daje genetski algoritam sa jednostavnim algoritmom grube sile. Algoritam za svako moguće rešenje izračunava preciznost učenja, što znači da je složenost \( O(2^n) \), dok je složenost genetskog algoritma polinomijalna, i zavisi od postavljenih parametara (broja generacija i velicine populacije). U slučaju predstavljenih algoritama, eksperimentalnom metodom došlo se do \lstinline{POPULATION_SIZE = 20} i \lstinline{MAX_ITER = 20} vrednosti. Veličina populacije treba da bude dovoljno velika da generiše dovoljno raznovrsne jedinke. Za broj iteracija je utvrđeno da je najmanji koji daje tačan, odnosno dovoljno dobar rezultat. Prikazaćemo vreme izvršavanja algoritma za svako \lstinline{n} , kao i vreme izvršavanja \lstinline{calculateFitness} funkcije za različito \lstinline{n}.
\newline Sledi kod algoritma grube sile:

\begin{lstlisting}
import itertools

def bruteForceAlg(n):
    numResources = n 
    bestResult = ([True for x in range(numResources)], 0.0)
    numFeatures = n
    lst = [list(i) for i in itertools.product([False, True], repeat=numResources)]
    # skip first element [0,....,0]
    for i in range(1, len(lst)):
        sample = lst[i]
        print(sample)
        accuracy = getModelAccuracy(sample)
        print(accuracy)
        if accuracy > bestResult[1]:
            numFeaturesSample = len([x for x in sample if x])
            bestResult = (sample, accuracy)
            numFeatures = numFeaturesSample
        elif accuracy == bestResult[1]:
            numFeaturesSample = len([x for x in sample if x])
            if numFeaturesSample < numFeatures:
                bestResult = (sample, accuracy)
                numFeatures = numFeaturesSample
                
    print('Solution: {}, accuracy: {}', bestResult[0], bestResult[1])
\end{lstlisting}

\begin{table}[h]
\begin{widepage}
\begin{tabular}{||c | c c c||} 
 \hline
 n & BruteForce & GenAlg & GenAlgSimulatedAnnealing \\ [0.5ex] 
 \hline\hline

  4 & 
  \multirow{3}{4.0cm}{\centering [0, 0, 1, 1] 0.6505746841430664 25.2s} &	
  \multirow{3}{4.0cm}{\centering [0, 0, 1, 1] 0.6620689630508423 2min 42s} &	
  \multirow{3}{4.0cm}{\centering [0, 0, 1, 1] 0.659770131111145 7min 18s} 
  \\ \\ \\ \hline
 5 & 
 \multirow{3}{4.0cm}{\centering [1, 0, 1, 1, 0] 0.657471239566803 42.8s} &	
 \multirow{3}{4.0cm}{\centering [1, 1, 0, 1, 1], 0.6666666865348816 2min 33s} &	
 \multirow{3}{4.0cm}{\centering [0, 0, 1, 1, 1], 0.6666666865348816 7min 29s} 
 \\ \\ \\ \hline
 6 & 
 \multirow{3}{4.0cm}{\centering [1, 0, 1, 1, 0, 1] 0.6643677949905396 1min27s} &	
 \multirow{3}{4.0cm}{\centering [0, 0, 1, 1, 1, 0], 0.6666666865348816 2min 26s} &	
 \multirow{3}{4.0cm}{\centering [1, 0, 1, 1, 0, 1], 0.659770131111145 7min 27s}
 \\ \\ \\ \hline
 7 & 
 \multirow{3}{4.0cm}{\centering [0, 0, 1, 0, 1, 0, 1] 0.6666666865348816 3min4s} &	
 \multirow{3}{4.0cm}{\centering [0, 0, 1, 0, 1, 1, 0], 0.6643677949905396 2min 30s} &	
 \multirow{3}{4.0cm}{\centering [1, 1, 0, 1, 0, 0, 1], 0.6643677949905396 7min 25s} 
 \\ \\ \\ \hline
 8 & 
 \multirow{3}{4.0cm}{\centering [1, 1, 1, 1, 0, 0, 0, 1] 0.6735632419586182 5min21s} &	
 \multirow{3}{4.0cm}{\centering [1, 0, 1, 1, 0, 0, 1, 0], 0.6712643504142761 2min 35s} &	
 \multirow{3}{4.0cm}{\centering [0, 1, 1, 1, 0, 0, 0, 1], 0.6689655184745789 7min 32s} 
 \\ \\ \\ \hline
 9 & 
 \multirow{3}{4.0cm}{\centering [1, 0, 1, 1, 0, 1, 0, 0, 0] 0.6712643504142761 12min 15s} &
 \multirow{3}{4.0cm}{\centering [1, 1, 1, 1, 0, 1, 1, 0, 0], 0.6689655184745789 2min 33s} &	
 \multirow{3}{4.0cm}{\centering [1, 0, 0, 1, 0, 1, 0, 1, 0], 0.6689655184745789 7min 24s}
  \\ \\ \\ \hline
 10 & 
 \multirow{3}{4.3cm}{\centering [1, 1, 0, 1, 0, 0, 0, 1, 0, 1] 0.6804597973823547 24min 1s} &	
 \multirow{3}{4.3cm}{\centering [1, 1, 0, 1, 0, 0, 0, 1, 0, 1], 0.6781609058380127 2min 40s} &	
 \multirow{3}{4.3cm}{\centering [0, 0, 1, 1, 0, 0, 0, 0, 0, 1], 0.6666666865348816 7min 33s} 
\\ \\ \\[1ex] 
 \hline
\end{tabular}
\caption{rezultati algoritama nad malim \lstinline{n}} 
\end{widepage}
\end{table}

\subsection{Diskusija rezultata}
Primetimo da se vreme izvršavanja algoritma grube sile značajno povećava za veće  \lstinline{n}. Zaključujemo da je algoritam nemoguće koristiti da veću dimenziju ulaza. Sa druge strane, genetski algoritam se izvršava u konstantom vremenu - menjanjem broja iteracija ili veličine populacije ovo vreme će varirati, ali vidimo da je algoritam koristan i za veliko  \lstinline{n}. Slično i za simulirano kaljenje, koje se duže izvršava zbog dodatnih komputacija, ali i dalje u konstantnom vremenu.\newline
Treba uzeti u obzir da trenirani model za isti ulaz neće uvek vratiti isti izlaz. U slučajevima gde više podskupova ima jako slične performanse, ne znamo koji od njih će nam algoritam vratiti. Da bismo se osigurali da dobijemo najmanji mogući podskup, možemo da gledamo preciznost kao opseg, i fitnes računamo uzimajuci veličinu skupa u obzir. Predstavićemo i rezultate algoritma sa izmenjenim računanjem fitnesa.\newpage

\begin{table}[h]
\centering
\begin{tabular}{||c | c c ||} 
 \hline
 n & GenAlg & GenAlg (expanded fitness) \\ [0.5ex] 
 \hline\hline
  5 & 
  \multirow{3}{4.0cm}{\centering [0, 0, 1, 0, 1] 0.6620689630508423 2min 58s} &	
  \multirow{3}{4.0cm}{\centering [0, 0, 1, 0, 0] 0.634482741355896 2min 54s} 
  \\ \\ \\ \hline
 10 & 
 \multirow{3}{4.5cm}{\centering [1, 1, 1, 1, 0, 0, 0, 1, 0, 0], 0.6781609058380127 \newline 2min 40s} &	
 \multirow{3}{4.5cm}{\centering[1, 0, 0, 0, 0, 1, 0, 1, 0, 0], 0.6505746841430664 \newline 2min 45s} 
 \\ \\ \\ \hline
 20 & 
 \multirow{4}{4.8cm}{\centering [0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1] \newline 0.7195402383804321 \newline 2min 49s} &	
 \multirow{4}{4.8cm}{\centering [0, 0, 1, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0] \newline 0.707011517683665 \newline 2min 57s} 
 \\  \\ \\ \\ \hline
\end{tabular}
\caption{rezultati genetskog algoritma sa proširenim računanjem fitnesa} 
\end{table}

Vidimo da u nekim slučajevima možemo da dobijemo dosta manje skupove atributa koji imaju sličnu preciznost. Na primeru \lstinline{n=20} rezultat običnog i proširenog algoritma je isti (ista veličina skupa), pa možemo zaključiti da je podksup veličine 7 sigurno dobro rešenje.

\begin{table}[h]
\centering
\begin{tabular}{||c | c c ||} 
 \hline
 n & model & model + attribute set size \\ [0.5ex] 
 \hline\hline
 5 & 2.11s & 1.72s
  \\ \hline
 10 & 1.71s & 1.72s
 \\  \hline
 20 & 1.73s & 1.75s
 \\  \hline
\end{tabular}
\caption{vreme izvršavanja računanja fitnesa samo sa modelom, i sa modelom + brojem atributa za različito \lstinline{n}} 
\end{table}
	
\newpage
\section{Zaključak}
Ukoliko analiziramo same atribute, ne treba gledati vrednost pojedinačnih atributa, već veze između njih, odosno vrednost kombinacija atributa. \newline
Mana predloženog algoritma je u ceni računanja fitnesa jedinke. Za svaku novu jedinku moramo da treniramo mašinu sa novim podskupom atributa, da bismo ocenili tačnost klasifikacije nad tim podskupom. Vreme trajanja ove operacije nije zanemarljivo, i bilo bi bolje na drugi način oceniti fitnes. \newline
Sa druge strane, genetski algoritam u razumljivo vreme daje rezultat koji može da parira priloženim rezultatima pohlepnog algoritma. Jedna varijacija algoritma bi mogla biti da ocena fitnesa jedinke bude kombinacija tačnosti klasifikacije i broja atributa, gde se prednost daje jedinkama sa manjim brojem atributa.
%\begin{figure}[h!]
%	\centering
%		\includegraphics[width=0.8\textwidth]{classification_print}
%		\caption{histogram cena i poena}
%	\end{figure}
%\begin{figure}[h!]
%	\centering
%		\includegraphics[width=0.8\textwidth]{conf_matrix_print}
%	\end{figure}
\newpage

\newpage
\begin{thebibliography}{9}

\bibitem{VanHorn} K. Van Horn and T. Martinez, \emph{The Minimum Feature Set Problem},  Neural Networks 7 (1994), no. 3, pp. 491-494.
\url{https://axon.cs.byu.edu/papers/vanhorn_3.pdf}.

\bibitem{Kira} K.Kira, L.Randell, \emph{The Feature Selection Problem: Traditional Methods and a New Algorithm}, AAAI-92 Proc., 10th International Conference on Artificial Intelligence, 1992.

\bibitem{Cui} M.Cui, S.Prasad, M.Mahrooghy, \emph{Genetic algorithms and Linear Discriminant Analysis based dimensionality reduction for remotely sensed image analysis},  2011 IEEE International Geoscience and Remote Sensing Symposium, available at 
\url{https://www.researchgate.net/publication/220819539_Genetic_algorithms_and_Linear_Discriminant_Analysis_based_dimensionality_reduction_for_remotely_sensed_image_analysis}.

\end{thebibliography}

\end{document}


























